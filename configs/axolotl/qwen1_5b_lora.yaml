base_model: Qwen/Qwen1.5-1.8B
load_in_4bit: true
adapter: lora
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
seq_len: 4096
micro_batch_size: 4
gradient_accumulation_steps: 8
optimizer: adamw_bnb_8bit
learning_rate: 2e-4
num_train_epochs: 1
save_steps: 200
dataset: configs/datasets/sample.jsonl
output_dir: /workspace/checkpoints/qwen1_5b_lora
