services:
  vllm:
    container_name: vllm
    image: vllm/vllm-openai:latest
    command: >
      --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B
      --max-model-len 32768
    environment:
      HF_TOKEN: ${HF_TOKEN}
    ports:
      - "8000:8000"
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    restart: unless-stopped

  ollama:
    container_name: ollama
    image: ollama/ollama:latest
    volumes:
      - ollama:/root/.ollama
      - ../:/workspace
    environment:
      OLLAMA_HOST: 0.0.0.0
    ports:
      - "11434:11434"
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    restart: unless-stopped

  api:
    container_name: hypernode-api
    build:
      context: ../
      dockerfile: docker/api.Dockerfile
    env_file: ../.env
    environment:
      VLLM_BASE_URL: ${VLLM_BASE_URL}
      OLLAMA_BASE_URL: ${OLLAMA_BASE_URL}
      S3_ENDPOINT: ${S3_ENDPOINT}
      S3_BUCKET: ${S3_BUCKET}
      S3_ACCESS_KEY: ${S3_ACCESS_KEY}
      S3_SECRET_KEY: ${S3_SECRET_KEY}
      S3_USE_SSL: ${S3_USE_SSL}
      API_PORT: ${API_PORT}
    ports:
      - "${API_PORT}:8080"
    volumes:
      - ../:/workspace
    depends_on:
      - vllm
      - ollama
      - minio
    restart: unless-stopped

  minio:
    container_name: minio
    image: quay.io/minio/minio:latest
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: ${S3_ACCESS_KEY}
      MINIO_ROOT_PASSWORD: ${S3_SECRET_KEY}
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio:/data
    restart: unless-stopped

volumes:
  ollama: {}
  minio: {}
