# ğŸ§  Hypernode AI Deployer  
> Distributed AI Model Deployer â€” serving, fine-tuning, and managing LLMs across GPU nodes.


## ğŸš€ Overview

**Hypernode AI Deployer** is the orchestration layer for deploying, fine-tuning, and managing AI models across the **Hypernode Network** â€” a distributed GPU compute marketplace powered by Solana.  

It supports **inference, fine-tuning, and distributed training** for major open-source LLMs like **DeepSeek**, **Qwen**, **Mistral**, and **LLaMA**, with integration layers for **vLLM**, **Ollama**, and **Axolotl (QLoRA/PEFT)**.

---

## ğŸ§© Core Capabilities

| Feature | Description |
|----------|--------------|
| ğŸ” **Distributed Inference** | Run LLMs via **vLLM** (OpenAI-compatible endpoint) or **Ollama** (GGUF runtime). |
| ğŸ§® **Fine-tuning Pipeline** | Integrated **Axolotl** runner for PEFT/QLoRA training on local or distributed GPUs. |
| ğŸ’¾ **Checkpoint Management** | Automatic upload/download of models via **MinIO/S3**. |
| âš™ï¸ **API Layer** | REST endpoints for deployment, job tracking, and telemetry. |
| ğŸ§  **LLM Registry** | Declarative configs for DeepSeek, Qwen, and other models. |
| ğŸ›°ï¸ **Reward Integration** | Hooks for reward distribution and telemetry collection from the Hypernode core network. |

---

## ğŸ“¦ Components

```
â”œâ”€â”€ api/
â”‚   â””â”€â”€ main.py                # FastAPI service (deploy/list/status)
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ backends/
â”‚   â”‚   â”œâ”€â”€ vllm_backend.py    # Inference backend for DeepSeek/Qwen (vLLM)
â”‚   â”‚   â””â”€â”€ ollama_backend.py  # GGUF runtime (Ollama)
â”‚   â”œâ”€â”€ trainers/
â”‚   â”‚   â””â”€â”€ axolotl_runner.py  # Fine-tuning orchestrator (QLoRA)
â”‚   â”œâ”€â”€ checkpoint_store.py    # MinIO/S3 upload utilities
â”‚   â””â”€â”€ model_registry.py      # Default model configs
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ models/                # Model configuration files
â”‚   â”œâ”€â”€ axolotl/               # Fine-tuning configs
â”‚   â””â”€â”€ datasets/              # Example training data
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ docker-compose.yaml    # vLLM, Ollama, API, MinIO stack
â”‚   â”œâ”€â”€ api.Dockerfile
â”‚   â”œâ”€â”€ ollama/Modelfile       # Ollama model definition
â”‚   â””â”€â”€ vllm.Dockerfile
â””â”€â”€ scripts/
    â””â”€â”€ train_qwen1_5b.sh      # Axolotl training helper
```

---

## âš™ï¸ Setup & Deployment

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/Hypernode-sol/Hypernode-AI-Deployer.git
cd Hypernode-AI-Deployer
```

### 2ï¸âƒ£ Environment configuration
Create your `.env` file:
```bash
cp .env.example .env
```
Edit and fill with your credentials:
```bash
HF_TOKEN=your_huggingface_token_here
S3_ENDPOINT=http://minio:9000
S3_BUCKET=hypernode-checkpoints
S3_ACCESS_KEY=minioadmin
S3_SECRET_KEY=minioadmin
S3_USE_SSL=false
API_PORT=8080
```

### 3ï¸âƒ£ Launch stack
Run all containers (vLLM, Ollama, FastAPI, MinIO):
```bash
docker compose -f docker/docker-compose.yaml --env-file .env up -d --build
```

---

## ğŸ§  Inference Examples

### DeepSeek-R1-Distill (vLLM)
```bash
curl http://localhost:8000/v1/chat/completions   -H "Content-Type: application/json"   -d '{
    "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
    "messages": [{"role":"user","content":"Explain Hypernode in one line."}]
  }'
```

### Qwen via Ollama
```bash
docker exec -it ollama ollama create qwen2.5-7b-instruct -f /workspace/docker/ollama/Modelfile
docker exec -it ollama ollama run qwen2.5-7b-instruct
```

---

## ğŸ”§ Fine-tuning (QLoRA / PEFT)

Train Qwen 1.5B using Axolotl:
```bash
bash scripts/train_qwen1_5b.sh
```

Modify the config:
```yaml
# configs/axolotl/qwen1_5b_lora.yaml
base_model: Qwen/Qwen1.5-1.8B
load_in_4bit: true
adapter: lora
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
seq_len: 4096
dataset: configs/datasets/sample.jsonl
output_dir: /workspace/checkpoints/qwen1_5b_lora
```

Resulting adapters are stored in:
```
./checkpoints/qwen1_5b_lora/
```
You can attach these LoRAs to Ollama models by editing:
```dockerfile
# docker/ollama/Modelfile
ADAPTER ./checkpoints/qwen1_5b_lora.gguf
```

---

## ğŸŒ API Endpoints

| Method | Endpoint | Description |
|--------|-----------|--------------|
| `GET` | `/api/list_models` | Lists available models from vLLM and Ollama. |
| `POST` | `/api/deploy_model` | Deploys or warms up a model. |
| `GET` | `/api/get_job_status?job_id=` | Returns training/deployment job status. |

Example:
```bash
curl -X POST http://localhost:8080/api/deploy_model   -H "Content-Type: application/json"   -d '{
    "base_model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
    "backend": "vllm"
  }'
```

---

## ğŸ–¥ï¸ Monitoring & Storage

- **MinIO Dashboard:**  
  â†’ http://localhost:9001 (user: `minioadmin`, pass: `minioadmin`)

- **vLLM Endpoint:**  
  â†’ http://localhost:8000/v1

- **Ollama Endpoint:**  
  â†’ http://localhost:11434

- **API Service:**  
  â†’ http://localhost:8080/docs (Swagger UI)

---

## ğŸ§© Architecture Diagram

```text
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚        User / Client         â”‚
               â”‚     (API / Dashboard UI)     â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚     Hypernode API Layer      â”‚
               â”‚  deploy_model / list_models  â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                 â”‚                      â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   vLLM        â”‚   â”‚   Ollama    â”‚      â”‚  Axolotl/PEFT   â”‚
   â”‚ DeepSeek/Qwen â”‚   â”‚ GGUF models â”‚      â”‚ Fine-tune jobs  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                  â”‚                     â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                     â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
                     â”‚  MinIO/S3 â”‚
                     â”‚ Checkpointsâ”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ” License

This project is licensed under the **MIT License**.  
Models such as **DeepSeek R1 Distill** and **Qwen** follow their respective open-source licenses (MIT / Apache 2.0).

---

## ğŸŒ About Hypernode

Hypernode is a **distributed GPU compute network** that connects idle compute power with AI workloads through tokenized incentives and intelligent orchestration layers.  
Learn more at **[hypernodesolana.org](https://hypernodesolana.org)**.

---

### ğŸ¤ Contributors
- Core Engineering: **Hypernode Labs**
- AI Infrastructure: **Hypernode Network Team**
- Systems Integration: **Hypernode Compute Layer**

---

**Hypernode Â© 2025 â€” Building the distributed intelligence network of tomorrow.**

